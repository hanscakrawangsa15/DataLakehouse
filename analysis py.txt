# ✅ ANALYZE: Membersihkan & Menstrukturkan Data Mentah ke Staging Database

"""
Tujuan:
- Parsing isi file (PDF → Teks → DataFrame)
- Validasi tipe data dan handling missing values
- Cleaning teks, normalisasi data (contoh: lowercase, hapus karakter khusus)
- Deteksi entitas penting (contoh: nama perusahaan, sentimen)
- Simpan hasil analisis ke database staging
"""

import pandas as pd
import fitz  # PyMuPDF
from sqlalchemy import create_engine, exc
import re
from pathlib import Path
from datetime import datetime
import logging

# Konfigurasi logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ---------- KONFIGURASI PATH & DATABASE ---------- #
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data_lake" / "adventureworks" / "organized"
OUTPUT_DIR = BASE_DIR / "data_lake" / "adventureworks" / "processed"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Konfigurasi database
DB_CONFIG = {
    'dbname': 'staging',
    'user': 'postgres',
    'password': 'chriscakra15',
    'host': 'localhost',
    'port': '5432'
}

def get_database_connection():
    """Membuat koneksi ke database"""
    try:
        conn_str = f"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}"
        return create_engine(conn_str)
    except Exception as e:
        logger.error(f"Gagal terhubung ke database: {e}")
        raise

def process_csv_file(csv_path):
    """Memproses file CSV"""
    try:
        logger.info(f"Memproses file CSV: {csv_path}")
        df = pd.read_csv(csv_path)
        
        # Handle warehouse temperature sensor data
        if 'warehouse_temp_sensor' in str(csv_path):
            # Pastikan kolom yang diperlukan ada
            required = ['timestamp', 'sensor_id', 'temperature_c']
            missing = [col for col in required if col not in df.columns]
            if missing:
                logger.error(f"Kolom yang diperlukan tidak ditemukan: {missing}")
                return None, None
                
            # Pastikan tipe data sesuai
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            df['temperature_c'] = pd.to_numeric(df['temperature_c'], errors='coerce')
            
            # Hapus baris dengan data yang tidak valid
            df = df.dropna(subset=['timestamp', 'sensor_id', 'temperature_c'])
        
        # Simpan ke CSV yang sudah diproses
        output_path = OUTPUT_DIR / f"processed_{csv_path.name}"
        df.to_csv(output_path, index=False)
        
        return df, output_path
    except Exception as e:
        logger.error(f"Gagal memproses file CSV {csv_path}: {e}")
        return None, None

def process_txt_file(txt_path):
    """Memproses file teks (tweet data)"""
    try:
        logger.info(f"Memproses file TXT: {txt_path}")
        
        # Baca dan parse data tweet
        df = pd.read_csv(
            txt_path, 
            sep='\t', 
            header=None, 
            names=[
                'tweet_id', 
                'tweet_text', 
                'timestamp', 
                'user_location', 
                'sentiment', 
                'matched_product'
            ]
        )
        
        # Validasi kolom yang diperlukan
        required = ['tweet_id', 'tweet_text', 'timestamp', 'user_location', 'sentiment', 'matched_product']
        missing = [col for col in required if col not in df.columns]
        if missing:
            logger.error(f"Kolom yang diperlukan tidak ditemukan: {missing}")
            return None, None
        
        # Cleaning dan validasi data
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        df['tweet_text'] = df['tweet_text'].str.strip()
        df['user_location'] = df['user_location'].fillna('Unknown').str.strip()
        df['sentiment'] = df['sentiment'].str.lower().str.strip()
        df['matched_product'] = df['matched_product'].fillna('Unknown').str.strip()
        
        # Hapus baris dengan data yang tidak valid
        df = df.dropna(subset=['tweet_id', 'tweet_text', 'timestamp', 'sentiment'])
        
        # Simpan ke CSV yang sudah diproses
        output_path = OUTPUT_DIR / f"tweet_analysis_{datetime.now().strftime('%Y%m%d')}.csv"
        df.to_csv(output_path, index=False)
        
        return df, output_path
    except Exception as e:
        logger.error(f"Gagal memproses file TXT {txt_path}: {e}")
        return None, None

def process_market_share_pdf(pdf_path):
    """
    Memproses file PDF laporan market share dan mengekstrak data:
    - competitor
    - market_share_percent
    - time_period
    - extraction_date
    """
    try:
        logger.info(f"Memproses laporan market share: {pdf_path}")
        
        def extract_table_from_pdf(path):
            """Mengekstrak tabel dari PDF"""
            with fitz.open(path) as doc:
                all_tables = []
                for page in doc:
                    # Dapatkan tabel dari halaman
                    tables = page.find_tables()
                    if tables.tables:
                        all_tables.extend([table.extract() for table in tables])
            return all_tables
        
        def find_market_share_table(tables):
            """Mencari tabel yang berisi data market share"""
            for table in tables:
                # Cari header yang sesuai
                headers = [str(cell).lower() for row in table[:1] for cell in row]
                header_text = ' '.join(headers)
                
                # Cari header yang mengandung kata kunci
                if any(keyword in header_text for keyword in ['time periode','competitor','market share']):
                    return table
            return None

        def parse_table_data(table):
            """Memproses data tabel menjadi DataFrame"""
            # Dapatkan header dan cari indeks kolom yang dibutuhkan
            headers = [str(cell).lower().strip() for cell in table[0]]
            
            # Cari indeks kolom yang sesuai
            period_col = next((i for i, h in enumerate(headers) if 'period' in h or 'time' in h), None)
            comp_col = next((i for i, h in enumerate(headers) if 'competitor' in h or 'company' in h), None)
            share_col = next((i for i, h in enumerate(headers) if 'market' in h and 'share' in h), None)
            
            if None in (period_col, comp_col, share_col):
                return pd.DataFrame()
            
            # Proses setiap baris data
            data = []
            for row in table[1:]:  # Lewati header
                if len(row) > max(period_col, comp_col, share_col):
                    try:
                        period = str(row[period_col]).strip()
                        competitor = str(row[comp_col]).strip()
                        market_share = float(str(row[share_col]).replace('%', '').strip())
                        
                        if competitor and market_share > 0:
                            data.append({
                                'time_periode': period,
                                'competitor': competitor,
                                'market_share_percent': market_share
                            })
                    except (ValueError, IndexError):
                        continue
            
            return pd.DataFrame(data)
        
        # Ekstrak dan proses tabel
        tables = extract_table_from_pdf(pdf_path)
        if not tables:
            logger.warning("Tidak ada tabel yang ditemukan dalam PDF")
            return None, None
            
        market_share_table = find_market_share_table(tables)
        if market_share_table is None:
            logger.warning("Tabel market share tidak ditemukan dalam PDF")
            return None, None
            
        df = parse_table_data(market_share_table)
        
        if df.empty:
            logger.warning("Tidak ada data market share yang berhasil diekstraksi dari tabel")
            return None, None
            
        # Pastikan kolom yang diperlukan ada
        required_columns = ['competitor', 'market_share_percent', 'time_periode']
        for col in required_columns:
            if col not in df.columns:
                logger.error(f"Kolom {col} tidak ditemukan dalam data yang diekstrak")
                return None, None
            
        # Tambahkan extraction_date
        df['extraction_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # Simpan ke CSV
        output_path = OUTPUT_DIR / f"market_share_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(output_path, index=False)
        logger.info(f"Data market share berhasil disimpan ke: {output_path}")
            
        return df, output_path
        
    except Exception as e:
        logger.error(f"Gagal memproses laporan market share: {e}", exc_info=True)
        return None, None

def load_to_database(df, table_name):
    """Memuat data ke database staging"""
    try:
        engine = get_database_connection()
        
        # Pastikan kolom yang diperlukan ada
        required_columns = {
            'staging_market_share_report': ['time_periode', 'competitor', 'market_share_percent', 'extraction_date'],
            'staging_warehouse_temp_sensor': ['timestamp', 'sensor_id', 'temperature_c'],
            'staging_external_sentiment': ['tweet_id', 'tweet_text', 'timestamp', 'user_location', 'sentiment', 'matched_product']
        }
        
        # Validasi kolom yang diperlukan
        if table_name in required_columns:
            # Tambahkan extraction_date untuk market share report jika belum ada
            if table_name == 'staging_market_share_report' and 'extraction_date' not in df.columns:
                df['extraction_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
            missing_cols = [col for col in required_columns[table_name] if col not in df.columns]
            if missing_cols:
                logger.error(f"Kolom yang diperlukan tidak ditemukan: {missing_cols}")
                return False
        
        # Pastikan tipe data sesuai
        if 'market_share_percent' in df.columns:
            df['market_share_percent'] = pd.to_numeric(df['market_share_percent'], errors='coerce')
            df = df.dropna(subset=['market_share_percent'])
        
        # Load ke database
        with engine.connect() as conn:
            # Gunakan if_exists='append' untuk menambahkan data ke tabel yang sudah ada
            df.to_sql(
                name=table_name,
                con=conn,
                if_exists='append',
                index=False,
                method='multi',
                chunksize=1000
            )
            
            # Commit transaksi
            conn.commit()
        
        logger.info(f"Berhasil memuat {len(df)} baris ke tabel {table_name}")
        return True
        
    except Exception as e:
        logger.error(f"Gagal memuat data ke database: {e}", exc_info=True)
        if 'conn' in locals():
            conn.rollback()
        return False

def main():
    try:
        logger.info("Memulai proses analisis data...")
        
        # Pastikan direktori data ada
        if not DATA_DIR.exists():
            logger.error(f"Direktori data tidak ditemukan: {DATA_DIR}")
            return
        
        # Proses file sensor suhu gudang
        sensor_file = DATA_DIR / "warehouse_temp_sensor.csv"
        if sensor_file.exists():
            try:
                logger.info(f"Memproses file sensor: {sensor_file}")
                df_sensor, _ = process_csv_file(sensor_file)
                
                # Load to database
                if load_to_database(df_sensor, "staging_warehouse_temp_sensor"):
                    logger.info("✅ Data sensor berhasil dimuat ke staging_warehouse_temp_sensor")
                else:
                    logger.error("❌ Gagal memuat data sensor ke database")
            except Exception as e:
                logger.error(f"Gagal memproses file sensor: {e}", exc_info=True)
        else:
            logger.warning(f"File sensor tidak ditemukan: {sensor_file}")
        
        # Proses file tweet
        tweet_file = DATA_DIR / "adventureworks_structured_150_tweets.txt"
        if tweet_file.exists():
            try:
                logger.info(f"Memproses file tweet: {tweet_file}")
                df_tweet, _ = process_txt_file(tweet_file)
                
                # Load to database
                if load_to_database(df_tweet, "staging_external_sentiment"):
                    logger.info("✅ Data tweet berhasil dimuat ke staging_external_sentiment")
                else:
                    logger.error("❌ Gagal memuat data tweet ke database")
            except Exception as e:
                logger.error(f"Gagal memproses file tweet: {e}", exc_info=True)
        else:
            logger.warning(f"File tweet tidak ditemukan: {tweet_file}")
        
        # Proses file market share PDF
        market_share_file = DATA_DIR / "market_share_report.pdf"
        if market_share_file.exists():
            try:
                logger.info(f"Memproses file market share: {market_share_file}")
                df_market_share, _ = process_market_share_pdf(market_share_file)
                
                # Load to database
                if df_market_share is not None and load_to_database(df_market_share, "staging_market_share_report"):
                    logger.info("✅ Data market share berhasil dimuat ke staging_market_share_report")
                else:
                    logger.error("❌ Gagal memuat data market share ke database")
            except Exception as e:
                logger.error(f"Gagal memproses file market share: {e}", exc_info=True)
        else:
            logger.warning(f"File market share tidak ditemukan: {market_share_file}")
        
        logger.info("✅ Proses analisis data selesai")
    
    except Exception as e:
        logger.error(f"Terjadi kesalahan dalam proses analisis: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    main()
